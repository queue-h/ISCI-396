{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "560f8369-c6d2-4759-aed4-94adf958fb3d",
   "metadata": {},
   "source": [
    "## Problem 1: Galaxy Photometric Redshifts Continued (6 points)\n",
    "\n",
    "In this exercise, we will return to the galaxy redshifts from problem set 4, equipped with our new tools: gradient boosting and neural networks. Here is the basic information on the data.\n",
    "\n",
    "The file `Problem_Set_4_Redshifts.csv` contains the following data.\n",
    "* `<BAND>_FLUX`: Fluxes in different bands. This is what will be used for training. Note that the data has been cleaned, and galaxies with missing fluxes have been removed.\n",
    "* `Z_PHOT`: Ignore this column.\n",
    "* `Z_SPEC`: These are spectroscopic redshifts $z_\\mathrm{spec}$ measured from the Dark Energy Spectroscopic Instrument (DESI).\n",
    "\n",
    "(a) Use the `sklearn.ensemble.GradientBoostingRegressor` (default parameters) and compare the feature importance to that of `sklearn.ensemble.RandomForestRegressor` (default parameters). What are the top five most important features? Are the rankings similar?\n",
    "\n",
    "(b) Perform a $k$-fold cross-validation with $k=10$ to optimize the hyperparameters of `sklearn.ensemble.HistGradientBoostingRegressor` based on the $R^2$ score. Include (+/- 1 standard deviation) error bars derived from the scatter between different folds. For reference, the best-performing gradient boosting regressor performs at least as well as the the best random forest, i.e., $R^2 \\gtrsim 0.85$. Here are a few parameters you should explore.\n",
    "* `learning_rate`\n",
    "* `max_iter`\n",
    "* `early_stopping`\n",
    "* `max_leaf_nodes`\n",
    "\n",
    "(Tip: Use `n_jobs=-1` in `sklearn.model_selection.GridSearchCV` to parallelize cross-validation.)\n",
    "\n",
    "(c) Repeat exercise (b) for `sklearn.neural_network.MLPRegressor`. Again, you should aim for an $R^2$ validation score of $R^2 \\gtrsim 0.85$. Here are a few parameters you might vary.\n",
    "* `early_stopping`\n",
    "* `hidden_layer_sizes`\n",
    "* `alpha`\n",
    "* `activation`\n",
    "\n",
    "I suggest you start with $3$ hidden layers of $100$ neurons each (`hidden_layer_sizes=(100, 100, 100)`). (Hint: Don't forget to scale the input features, for example, via `sklearn.preprocessing.StandardScaler`.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
